# EgoTextVQA: Towards Egocentric Scene-Text Aware Video Question Answering

![Task](https://img.shields.io/badge/Task-TextVideoQA-red)
![Task](https://img.shields.io/badge/Task-Egocentric--QA-orange)
![Dataset](https://img.shields.io/badge/Dataset-EgoTextVQA-blue)
![Model](https://img.shields.io/badge/Model-Gemini--1.5--Pro-green)
![Model](https://img.shields.io/badge/Model-Gemini--1.5--Flash-green)
![Model](https://img.shields.io/badge/Model-GPT--4o-green)


---

## ![EgoTextVQA](https://github.com/zhousheng97/EgoTextVQA/blob/main/asset/logo.png)  

<p align="center">
  [<a href="https://project-page-link.com">ğŸŒ Project Page</a>]
  [<a href="https://arxiv.org/abs/xxxx">ğŸ“– arXiv Paper</a>]
  [<a href="https://dataset-link.com">ğŸ“Š Dataset</a>]
  [<a href="https://leaderboard-link.com">ğŸ† Leaderboard</a>]
</p>


---

EgoTextVQA applies to both **image MLLMs**, i.e., generalizing to multiple images, and **video MLLMs**. ğŸ‰

---
## ğŸ”¥ News
- `2025.xx.xx` We are very proud to launch âœ¨**EgoTextVQA**âœ¨, a novel and rigorously constructed benchmark for egocentric QA assistance involving scene text!


