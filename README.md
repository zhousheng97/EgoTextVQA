# EgoTextVQA: Towards Egocentric Scene-Text Aware Video Question Answering

![Task](https://img.shields.io/badge/Task-TextVideoQA-red)
![Task](https://img.shields.io/badge/Task-Egocentric--QA-orange)
![Dataset](https://img.shields.io/badge/Dataset-EgoTextVQA-blue)
![Model](https://img.shields.io/badge/Model-Gemini--1.5--Pro-green)
![Model](https://img.shields.io/badge/Model-Gemini--1.5--Flash-green)
![Model](https://img.shields.io/badge/Model-GPT--4o-green)


---

## ![EgoTextVQA](https://raw.githubusercontent.com/path/to/logo.png)  
[[ğŸ Project Page ](https://project-page-link.com)] [[ğŸ“– arXiv Paper](https://arxiv.org/abs/xxxx)] [[ğŸ“Š Dataset](https://dataset-link.com)] [[ğŸ† Leaderboard](https://leaderboard-link.com)]


---

EgoTextVQA applies to both **image MLLMs**, i.e., generalizing to multiple images, and **video MLLMs**. ğŸ‰
